{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":255204,"sourceType":"datasetVersion","datasetId":107062}],"dockerImageVersionId":29186,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/himu-book')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/himu-book/himu.txt', 'r') as f:\n    text = f.read()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text[:500]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chars = tuple(set(text))\nint2char = dict(enumerate(chars))\nchar2int = {ch: ii for ii, ch in int2char.items()}\n\n# encode the text\nencoded = np.array([char2int[ch] for ch in text])\n\nencoded[:100]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode(arr, n_labels):\n    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n    \n    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n    \n    one_hot = one_hot.reshape((*arr.shape, n_labels))\n    \n    return one_hot","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_seq = np.array([[3, 5, 1]])\none_hot = one_hot_encode(test_seq, 8)\n\nprint(one_hot)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_batches(arr, batch_size, seq_length):\n    \n    batch_size_total = batch_size * seq_length\n    \n    n_batches = len(arr)//batch_size_total\n    arr = arr[:n_batches * batch_size_total]\n    arr = arr.reshape((batch_size, -1))\n    \n    for n in range(0, arr.shape[1], seq_length):\n        x = arr[:, n:n+seq_length]\n        y = np.zeros_like(x)\n        try:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n        except IndexError:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n        yield x, y","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batches = get_batches(encoded, 10, 50)\nx, y = next(batches)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('x\\n', x[:10, :10])\nprint('\\ny\\n', y[:10, :10])","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CharRNN(nn.Module):\n    \n    def __init__(self, tokens, n_hidden=256, n_layers=2,\n                               drop_prob=0.4, lr=0.001):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        \n        self.chars = tokens\n        self.int2char = dict(enumerate(self.chars))\n        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n        \n        self.lstm1 = nn.LSTM(len(self.chars), n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(drop_prob)\n        \n        self.fc1 = nn.Linear(n_hidden, int(n_hidden/2))\n        self.fc2 = nn.Linear(int(n_hidden/2), int(n_hidden/2))\n        self.fc3 = nn.Linear(int(n_hidden/2), len(self.chars))\n      \n    \n    def forward(self, x, hidden):\n        \n        r_output, hidden = self.lstm1(x, hidden)\n        \n        out = self.dropout(r_output)\n        \n        out = out.contiguous().view(-1, self.n_hidden)\n        \n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        \n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        \n        return hidden","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define and print the net\nn_hidden=1024\nn_layers=5\n\nnet = CharRNN(chars, n_hidden, n_layers)\nprint(net)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(net, data, epochs=10, batch_size=10, seq_length=10, lr=0.001, clip=5, val_frac=0.15, print_every=10):\n    \n    net.train()\n    \n    opt = torch.optim.Adam(net.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    # create training and validation data\n    val_idx = int(len(data)*(1-val_frac))\n    data, val_data = data[:val_idx], data[val_idx:]\n    \n    if(train_on_gpu):\n        net.cuda()\n    \n    counter = 0\n    n_chars = len(net.chars)\n    for e in range(epochs):\n        # initialize hidden state\n        h = net.init_hidden(batch_size)\n        \n        for x, y in get_batches(data, batch_size, seq_length):\n            counter += 1\n            \n            # One-hot encode our data and make them Torch tensors\n            x = one_hot_encode(x, n_chars)\n            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n            \n            if(train_on_gpu):\n                inputs, targets = inputs.cuda(), targets.cuda()\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # zero accumulated gradients\n            net.zero_grad()\n            \n            # get the output from the model\n            output, h = net(inputs, h)\n            \n            # calculate the loss and perform backprop\n            loss = criterion(output, targets.view(batch_size*seq_length).long())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            opt.step()\n            \n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for x, y in get_batches(val_data, batch_size, seq_length):\n                    # One-hot encode our data and make them Torch tensors\n                    x = one_hot_encode(x, n_chars)\n                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n                    \n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n                    \n                    inputs, targets = x, y\n                    if(train_on_gpu):\n                        inputs, targets = inputs.cuda(), targets.cuda()\n\n                    output, val_h = net(inputs, val_h)\n                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n                \n                    val_losses.append(val_loss.item())\n                \n                net.train() # reset to train mode after iterationg through validation data\n                \n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nseq_length = 10\nn_epochs = 30\n\n# train the model\ntrain(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(net, char, h=None, top_k=None):\n        ''' Given a character, predict the next character.\n            Returns the predicted character and the hidden state.\n        '''\n        \n        # tensor inputs\n        x = np.array([[net.char2int[char]]])\n        x = one_hot_encode(x, len(net.chars))\n        inputs = torch.from_numpy(x)\n        \n        if(train_on_gpu):\n            inputs = inputs.cuda()\n        \n        # detach hidden state from history\n        h = tuple([each.data for each in h])\n        # get the output of the model\n        out, h = net(inputs, h)\n\n        # get the character probabilities\n        p = F.softmax(out, dim=1).data\n        if(train_on_gpu):\n            p = p.cpu() # move to cpu\n        \n        # get top characters\n        if top_k is None:\n            top_ch = np.arange(len(net.chars))\n        else:\n            p, top_ch = p.topk(top_k)\n            top_ch = top_ch.numpy().squeeze()\n        \n        # select the likely next character with some element of randomness\n        p = p.numpy().squeeze()\n        char = np.random.choice(top_ch, p=p/p.sum())\n        \n        # return the encoded value of the predicted char and the hidden state\n        return net.int2char[char], h","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample(net, size, prime='হিমু ', top_k=None):\n        \n    if(train_on_gpu):\n        net.cuda()\n    else:\n        net.cpu()\n    \n    net.eval() # eval mode\n    \n    # First off, run through the prime characters\n    chars = [ch for ch in prime]\n    h = net.init_hidden(1)\n    for ch in prime:\n        char, h = predict(net, ch, h, top_k=top_k)\n\n    chars.append(char)\n    \n    # Now pass in the previous character and get a new one\n    for ii in range(size):\n        char, h = predict(net, chars[-1], h, top_k=top_k)\n        chars.append(char)\n\n    return ''.join(chars)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sample(net, 1000, prime='হিমু ', top_k=5))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}